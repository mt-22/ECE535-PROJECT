# ECE535-PROJECT
## VLM-based Smart Baby Monitoring

### *Motivation*
Current baby monitors typically have limited functionality when it comes to alerting parents of events. Some may sense when the baby moves or makes a sound, but not the exact kind of movement or sound. 
Using Visual Language Models we can analyze both image/video and audio of the baby to output precise information about what the baby is doing in a quick and easy to digest fashion, for example a line 
of text sent to the parents' phone as a notificaiton. 
### *Design Goals / Deliverables*

### *System Blocks (tentative)*

### *Requirements*
Python, Computer with CUDA Capability, And/Or Github Codespaces
### *Member Responsibilities*

### *Timeline*

### *References*

Li, Chunyuan. "Large Multimodal Models: Notes on CVPR 2023 Tutorial." arXiv:2306.14895, 2023.
https://huggingface.co/docs/transformers/main/en/model_doc/llava


